{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bff2038",
   "metadata": {},
   "source": [
    "# Reducción de dimensionalidad\n",
    "\n",
    "**¿Qué es la reducción de dimensionalidad?**\n",
    "\n",
    "...\n",
    "\n",
    "**¿Por qué querríamos usar técnicas de reducción de dimensionalidad para transformar nuestros datos antes del entrenamiento?**\n",
    "\n",
    "La convocatoria de reducción de dimensionalidad nos permite:\n",
    "\n",
    "- Eliminar la colinealidad del espacio de características.\n",
    "\n",
    "- Acelerar el entrenamiento al reducir el número de características.\n",
    "\n",
    "- Reducir el uso de la memoria al reducir la cantidad de características.\n",
    "\n",
    "- Identificar las características latentes subyacentes que impactan múltiples características en el espacio original.\n",
    "\n",
    "**¿Por qué querríamos evitar las técnicas de reducción de dimensionalidad para transformar nuestros datos antes del entrenamiento?**\n",
    "\n",
    "La reducción de la dimensionalidad puede:\n",
    "\n",
    "- Agregar cálculo adicional innecesario.\n",
    "\n",
    "- Hacer que el modelo sea difícil de interpretar si las características latentes no son fáciles de entender.\n",
    "\n",
    "- Agregar complejidad al modelo pipeline.\n",
    "\n",
    "- Reducir el poder predictivo del modelo si se pierde demasiada señal.\n",
    "\n",
    "Algunos algoritmos de reducción de dimensionalidad populares son:\n",
    "\n",
    "1. Análisis de componentes principales (PCA) - utiliza una descomposición propia para transformar los datos de características originales en vectores propios linealmente independientes. A continuación, se seleccionan los vectores más importantes (con valores propios más altos) para representar las características en el espacio transformado.\n",
    "\n",
    "2. Factorización de matriz no negativa (NMF) - se puede utilizar para reducir la dimensionalidad de ciertos tipos de problemas y conservar más información que PCA.\n",
    "\n",
    "3. Técnicas de incrustación - por ejemplo, encontrar vecinos locales como se hace en la incrustación lineal local puede usarse para reducir la dimensionalidad.\n",
    "\n",
    "4. Técnicas de agrupamiento o centroide - cada valor se puede describir como un miembro de un grupo, una combinación lineal de grupos o una combinación lineal de centroides de grupo.\n",
    "\n",
    "Por mucho, el más popular es PCA y variaciones similares basadas en la descomposición propia.\n",
    "\n",
    "La mayoría de las técnicas de reducción de dimensionalidad tienen transformaciones inversas, pero la señal a menudo se pierde al reducir las dimensiones, por lo que la transformación inversa suele ser solo una aproximación de los datos originales.\n",
    "\n",
    "**¿Cómo seleccionamos el número de componentes principales necesarios para PCA?**\n",
    "\n",
    "La selección del número de características latentes que se van a retener se realiza normalmente inspeccionando el valor propio de cada vector propio. A medida que disminuyen los valores propios, también disminuye el impacto de la característica latente en la variable objetivo.\n",
    "\n",
    "Esto significa que los componentes principales con valores propios pequeños tienen un impacto pequeño en el modelo y pueden eliminarse.\n",
    "\n",
    "Hay varias reglas generales, pero una regla general es incluir los componentes principales más significativos que representen al menos el 95 % de la variación en las características."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
